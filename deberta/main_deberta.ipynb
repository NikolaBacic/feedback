{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import DebertaConfig, DebertaTokenizerFast\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from param_deberta import param\n",
    "from processing_deberta import preprocess, discourse_map\n",
    "from dataset_deberta import DebertaDataset\n",
    "from model_deberta import init_deberta\n",
    "\n",
    "sys.path.append('/home/backe/projects/feedback/')\n",
    "from utils import seed_everything, moving_average, score_feedback_comp\n",
    "\n",
    "seed_everything(param['random_seed'])\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144293, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [04:48<00:00, 53.99it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DebertaTokenizerFast.from_pretrained(param['model_name'])\n",
    "\n",
    "TRAIN_PATH = '../data/train_clean.csv'\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(train_df.shape)\n",
    "train_df.head()\n",
    "\n",
    "TEXT_FILES = os.listdir('../data/train')\n",
    "TEXT_FILES = [f'../data/train/{file}' for file in TEXT_FILES]\n",
    "\n",
    "text_data = dict()\n",
    "for file_path in TEXT_FILES:\n",
    "    with open(file_path, 'r') as file:\n",
    "        idx = os.path.basename(file_path).split('.txt')[0]\n",
    "        text_data[idx] = file.read()\n",
    "        \n",
    "data = preprocess(text_data, tokenizer, train_df)\n",
    "deberta_df = pd.DataFrame(data, columns=['id', 'input_ids', 'attention_mask', 'token_to_word', 'target'])\n",
    "folds_df = pd.read_csv('../data/folds.csv')\n",
    "deberta_df = deberta_df.merge(folds_df, on='id')\n",
    "deberta_df.to_csv('/DATA/backe/feedback/data/deberta_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DebertaTokenizerFast.from_pretrained(param['model_name'])\n",
    "\n",
    "TRAIN_PATH = '../data/train_clean.csv'\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(train_df.shape)\n",
    "train_df.head()\n",
    "\n",
    "TEXT_FILES = os.listdir('../data/train')\n",
    "TEXT_FILES = [f'../data/train/{file}' for file in TEXT_FILES]\n",
    "\n",
    "text_data = dict()\n",
    "for file_path in TEXT_FILES:\n",
    "    with open(file_path, 'r') as file:\n",
    "        idx = os.path.basename(file_path).split('.txt')[0]\n",
    "        text_data[idx] = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in tqdm(text_data.items()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = '4BB688100D15'\n",
    "text = text_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right strip the text\n",
    "text = text.rstrip()\n",
    "\n",
    "# 1. GET INPUTS\n",
    "inputs = tokenizer(text,\n",
    "                   add_special_tokens=True,\n",
    "                   return_offsets_mapping=True,\n",
    "                   return_length=True)    \n",
    "\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_offset(pos: tuple, text:str) -> tuple:\n",
    "    \n",
    "    if pos[0] == pos[1]:\n",
    "        return pos\n",
    "    elif text[pos[0]] == ' ':\n",
    "        new_start = pos[0] + 1\n",
    "        return (new_start, pos[1])\n",
    "    else:\n",
    "        return pos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['offset_mapping'] = [clean_offset(pos, text) for pos in inputs['offset_mapping']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into words\n",
    "words = text.split()\n",
    "\n",
    "token_to_word = [] # list to store token -> word mapping\n",
    "word_pos = 0 # starting word position\n",
    "\n",
    "tokens = inputs['input_ids'][1:-1]  # exclude <s> and </s> tokens\n",
    "start = 0\n",
    "end = 1\n",
    "\n",
    "for _ in tokens:\n",
    "\n",
    "    word = tokenizer.decode(tokens[start:end]).strip()\n",
    "\n",
    "    # if striped word is an empty string, that token doesn't belong to any word\n",
    "    if word == '':\n",
    "        token_to_word.append(-1)\n",
    "        start += 1\n",
    "        end += 1\n",
    "        continue\n",
    "\n",
    "    # still no match\n",
    "    # continue adding tokens\n",
    "    if word != words[word_pos]:\n",
    "        end += 1\n",
    "        token_to_word.append(word_pos)\n",
    "    # match \n",
    "    else:\n",
    "        token_to_word.append(word_pos)\n",
    "        start = end\n",
    "        end = start + 1\n",
    "        word_pos += 1\n",
    "\n",
    "# add -1 position for the <s> and </s> tokens        \n",
    "token_to_word = [-1] + token_to_word + [-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize target 0s (all Fillers)\n",
    "target = np.full(inputs['length'][0], 0)\n",
    "id_filt = (train_df['id'] == idx)\n",
    "sample_df = train_df[id_filt]\n",
    "sample_df[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper numpy array\n",
    "token_to_word_np = np.array(token_to_word)\n",
    "\n",
    "# iterate discourses\n",
    "for row in sample_df.iterrows():\n",
    "    discourse_type = row[1]['discourse_type']\n",
    "    start = row[1]['new_start']\n",
    "    end = row[1]['new_end']\n",
    "\n",
    "    # this discourse's token positions\n",
    "    # set their targets\n",
    "    discourse_pos = [True if ((pos[0] >= start) and (pos[1] <= end)) else False for pos in inputs['offset_mapping']]\n",
    "    target[discourse_pos] = discourse_map[discourse_type]\n",
    "\n",
    "    # special first word's token's target for Claim and Evidence\n",
    "    # set their target to Claim_S / Evidence_S \n",
    "    if (discourse_type == 'Claim') or (discourse_type == 'Evidence'):\n",
    "        first_word_id = int(row[1]['predictionstring'].split()[0])\n",
    "        target[token_to_word_np == first_word_id] = discourse_map[discourse_type + '_S']\n",
    "\n",
    "# tokens that doesn't belong to any word set to -1\n",
    "# easier this way at the end...\n",
    "target[token_to_word_np == -1] = -1\n",
    "target = list(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target[108:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target[65:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][108]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['offset_mapping'][65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[291:410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_string(inputs['input_ids'][111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'][111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
