{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: 0.690+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import LongformerTokenizerFast\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from param_longformer import param\n",
    "from processing_longformer import preprocess, discourse_map, discourse_map_reverse\n",
    "from dataset_longformer import LongformerDataset, Collate\n",
    "from model_longformer import init_model\n",
    "\n",
    "# postprocess\n",
    "from pp_longformer import decode_predictions\n",
    "\n",
    "sys.path.append('/home/backe/projects/feedback/')\n",
    "from utils import seed_everything, moving_average, score_feedback_comp\n",
    "\n",
    "seed_everything(param['random_seed'])\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = param['gpu_idx']\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS THE DATA\n",
    "\n",
    "# tokenizer = LongformerTokenizerFast.from_pretrained(param['model_name'])\n",
    "\n",
    "# TRAIN_PATH = '../data/train_clean.csv'\n",
    "# train_df = pd.read_csv(TRAIN_PATH)\n",
    "# print(train_df.shape)\n",
    "# train_df.head()\n",
    "\n",
    "# TEXT_FILES = os.listdir('../data/train')\n",
    "# TEXT_FILES = [f'../data/train/{file}' for file in TEXT_FILES]\n",
    "\n",
    "# text_data = dict()\n",
    "# for file_path in TEXT_FILES:\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         idx = os.path.basename(file_path).split('.txt')[0]\n",
    "#         text_data[idx] = file.read()\n",
    "        \n",
    "# data = preprocess(text_data, tokenizer, train_df)\n",
    "# longformer_df = pd.DataFrame(data, columns=['id', 'input_ids', 'attention_mask', 'token_to_word', 'target'])\n",
    "# folds_df = pd.read_csv('../data/folds.csv')\n",
    "# longformer_df = longformer_df.merge(folds_df, on='id')\n",
    "# longformer_df.to_csv('/DATA/backe/feedback/longformer_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# LOAD PREPROCESSED DATA\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(param['model_name'])\n",
    "\n",
    "# load saved processed data\n",
    "DATA_PATH = '/DATA/backe/feedback/longformer_preprocessed.csv'\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data['input_ids'] = data['input_ids'].apply(eval)\n",
    "data['attention_mask'] = data['attention_mask'].apply(eval)\n",
    "data['token_to_word'] = data['token_to_word'].apply(eval)\n",
    "data['target'] = data['target'].apply(eval)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = Collate(tokenizer, purpose='train')\n",
    "dataset = LongformerDataset(data, param, purpose='train')\n",
    "\n",
    "train_dataloader, val_dataloader = dataset.get_dataloaders(collate_fn, param['fold_idx'])\n",
    "print(len(train_dataloader), len(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/train.csv'\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "fold_ids = data.loc[data['kfold'] == param['fold_idx'], 'id']\n",
    "val_df = train_df[train_df['id'].isin(fold_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = model.train_eval_pipeline(train_dataloader, val_dataloader, val_df, score_feedback_comp, decode_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MOVING AVERAGE TRAIN LOSS\n",
    "losses = np.concatenate(losses)\n",
    "ma_loss = moving_average(losses, window_size=50)\n",
    "plt.rcParams['figure.figsize'] = (20, 5)\n",
    "plt.plot(ma_loss[0:])\n",
    "plt.hlines(0.4 , xmin=0, xmax=len(ma_loss[0:]), colors='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "\n",
    "# model.save_pretrained(param['save_dir'])\n",
    "# tokenizer.save_pretrained(param['save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
