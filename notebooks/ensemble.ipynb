{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble and postprocessing research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('/home/backe/projects/feedback/')\n",
    "from utils import seed_everything, moving_average, score_feedback_comp, calc_overlap\n",
    "from decoder_2000.candidates import decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOF per model/fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAL_DF\n",
    "TRAIN_PATH = '../data/train.csv'\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "with open('word_probs_lf.pickle', 'rb') as handle:\n",
    "    word_probs_lf = pickle.load(handle)\n",
    "    \n",
    "with open('word_probs_rl.pickle', 'rb') as handle:\n",
    "    word_probs_rl = pickle.load(handle)\n",
    "\n",
    "with open('word_probs_db.pickle', 'rb') as handle:\n",
    "    word_probs_db = pickle.load(handle)\n",
    "\n",
    "# lgbm decoder model\n",
    "with open('../decoder_2000/decoder_model.pickle', 'rb') as handle:\n",
    "    model_decoder = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs_all = {}\n",
    "\n",
    "for idx in word_probs_lf.keys():\n",
    "    \n",
    "    word_probs_all[idx] = np.mean((word_probs_rl[idx], word_probs_db[idx]), axis=0)\n",
    "        \n",
    "#     word_probs_all[idx] = word_probs_db[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = decode_predictions(model_decoder, word_probs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7091085728350814,\n",
       " {'Lead': 0.8401486988847584,\n",
       "  'Position': 0.7234334763948498,\n",
       "  'Claim': 0.6708228141583802,\n",
       "  'Evidence': 0.7664355992844365,\n",
       "  'Concluding Statement': 0.8655643421998562,\n",
       "  'Counterclaim': 0.5875758991125642,\n",
       "  'Rebuttal': 0.5097791798107255})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take just one fold\n",
    "\n",
    "fold_idx = 4\n",
    "\n",
    "# targets\n",
    "folds = pd.read_csv('../data/folds.csv')\n",
    "fold_ids = folds.loc[folds['kfold'] == fold_idx, 'id']\n",
    "fold_df = train_df[train_df['id'].isin(fold_ids)]\n",
    "\n",
    "# predictions\n",
    "fold_preds = preds_df[preds_df['id'].isin(fold_ids)]\n",
    "\n",
    "score_feedback_comp(fold_preds, fold_df, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAL_DF\n",
    "TRAIN_PATH = '../data/train.csv'\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "with open('word_probs_lf.pickle', 'rb') as handle:\n",
    "    word_probs_lf = pickle.load(handle)\n",
    "    \n",
    "with open('word_probs_rl.pickle', 'rb') as handle:\n",
    "    word_probs_rl = pickle.load(handle)\n",
    "\n",
    "with open('word_probs_db.pickle', 'rb') as handle:\n",
    "    word_probs_db = pickle.load(handle)\n",
    "    \n",
    "# lgbm decoder model\n",
    "with open('../decoder_2000/decoder_model.pickle', 'rb') as handle:\n",
    "    model_decoder = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "word_probs_all = dict()\n",
    "\n",
    "for idx in word_probs_lf.keys():\n",
    "    \n",
    "    word_probs_all[idx] = np.mean((word_probs_lf[idx], word_probs_rl[idx], word_probs_db[idx]), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = decode_predictions(model_decoder, word_probs_all)\n",
    "score_feedback_comp(preds_df, train_df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find bad examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = preds_df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse_type = 'Claim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = train_df.copy()\n",
    "pred_df = preds_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n",
    "                  ['id', 'predictionstring']].reset_index(drop=True)\n",
    "pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n",
    "                  ['id', 'predictionstring']].reset_index(drop=True)\n",
    "pred_df['pred_id'] = pred_df.index\n",
    "gt_df['gt_id'] = gt_df.index\n",
    "pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n",
    "gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n",
    "\n",
    "# Step 1. all ground truths and predictions for a given class are compared.\n",
    "joined = pred_df.merge(gt_df,\n",
    "                       left_on='id',\n",
    "                       right_on='id',\n",
    "                       how='outer',\n",
    "                       suffixes=('_pred','_gt')\n",
    "                      )\n",
    "overlaps = [calc_overlap(*args) for args in zip(joined.predictionstring_pred, \n",
    "                                                 joined.predictionstring_gt)]\n",
    "\n",
    "# 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "# and the overlap between the prediction and the ground truth >= 0.5,\n",
    "# the prediction is a match and considered a true positive.\n",
    "# If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "# we don't need to compute the match to compute the score\n",
    "TP = joined.loc[overlaps]['gt_id'].nunique()\n",
    "\n",
    "# 3. Any unmatched ground truths are false negatives\n",
    "# and any unmatched predictions are false positives.\n",
    "TPandFP = len(pred_df)\n",
    "TPandFN = len(gt_df)\n",
    "\n",
    "#calc microf1\n",
    "my_f1_score = 2*TP / (TPandFP + TPandFN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FALSE NEGATIVE\n",
    "joined[joined['predictionstring_pred'].isna()].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FALSE POSITIVE\n",
    "joined[joined['predictionstring_gt'].isna()].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
